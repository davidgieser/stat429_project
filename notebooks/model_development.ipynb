{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL1 - Predicting Funding Rate Direction**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of **Model 1** is to predict whether the funding rate will **increase** or **decrease** in the next time period. This directional prediction provides the foundational input for **Model 2** (volatility estimation) and **Model 3** (exact value prediction). Accurate direction forecasting is critical for developing robust trading strategies.\n",
    "\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "- Predict whether the funding rate will **go up** or **down** in the next period using classification models.\n",
    "\n",
    "\n",
    "### **Approach**\n",
    "\n",
    "- **Input Features**:\n",
    "  - Historical funding rates and their lags.\n",
    "  - Technical indicators (e.g., moving averages, rate-of-change).\n",
    "  - Market features (e.g., open interest, mark price).\n",
    "  - Temporal features (e.g., hour, day).\n",
    "\n",
    "- **Models Tested**:\n",
    "  - **Logistic Regression**: A simple baseline for binary classification.\n",
    "  - **Random Forest Classifier**: Captures non-linear relationships and interactions.\n",
    "  - **Hyperparameter Tuning**: Applied to optimize model performance.\n",
    "\n",
    "- **Evaluation Metrics**:\n",
    "  - **Accuracy**: Correctly predicted directions.\n",
    "  - **F1 Score**: Handles imbalanced datasets.\n",
    "  - **ROC AUC**: Measures the model's ability to distinguish classes.\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "- Use the predicted direction from **Model 1** as an input feature for **Model 2** to forecast funding rate volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Data loaded. Preview:\n",
      "          exchange   symbol  local_timestamp  funding_timestamp  funding_rate  \\\n",
      "0  binance-futures  BTCUSDT     1.577840e+15       1.577866e+15     -0.000161   \n",
      "1  binance-futures  BTCUSDT     1.577844e+15       1.577866e+15     -0.000173   \n",
      "2  binance-futures  BTCUSDT     1.577848e+15       1.577866e+15     -0.000183   \n",
      "3  binance-futures  BTCUSDT     1.577851e+15       1.577866e+15     -0.000191   \n",
      "4  binance-futures  BTCUSDT     1.577855e+15       1.577866e+15     -0.000187   \n",
      "\n",
      "   predicted_funding_rate  open_interest  last_price  index_price  \\\n",
      "0                     NaN            NaN     7171.55          NaN   \n",
      "1                     NaN            NaN     7208.32          NaN   \n",
      "2                     NaN            NaN     7237.49          NaN   \n",
      "3                     NaN            NaN     7221.55          NaN   \n",
      "4                     NaN            NaN     7214.00          NaN   \n",
      "\n",
      "    mark_price         timestamp  \n",
      "0  7176.918847  1577836800000000  \n",
      "1  7213.227865  1577840400000000  \n",
      "2  7239.762514  1577844000000000  \n",
      "3  7224.738329  1577847600000000  \n",
      "4  7219.013281  1577851200000000  \n",
      "Dataset loaded successfully. Shape: (42179, 11)\n",
      "          exchange   symbol  local_timestamp  funding_timestamp  funding_rate  \\\n",
      "0  binance-futures  BTCUSDT     1.577840e+15       1.577866e+15     -0.000161   \n",
      "1  binance-futures  BTCUSDT     1.577844e+15       1.577866e+15     -0.000173   \n",
      "2  binance-futures  BTCUSDT     1.577848e+15       1.577866e+15     -0.000183   \n",
      "3  binance-futures  BTCUSDT     1.577851e+15       1.577866e+15     -0.000191   \n",
      "4  binance-futures  BTCUSDT     1.577855e+15       1.577866e+15     -0.000187   \n",
      "\n",
      "   predicted_funding_rate  open_interest  last_price  index_price  \\\n",
      "0                     NaN            NaN     7171.55          NaN   \n",
      "1                     NaN            NaN     7208.32          NaN   \n",
      "2                     NaN            NaN     7237.49          NaN   \n",
      "3                     NaN            NaN     7221.55          NaN   \n",
      "4                     NaN            NaN     7214.00          NaN   \n",
      "\n",
      "    mark_price         timestamp  \n",
      "0  7176.918847  1577836800000000  \n",
      "1  7213.227865  1577840400000000  \n",
      "2  7239.762514  1577844000000000  \n",
      "3  7224.738329  1577847600000000  \n",
      "4  7219.013281  1577851200000000  \n",
      "Preprocessing the data...\n",
      "Preprocessing the data completed. Shape: (42179, 9)\n",
      "   local_timestamp  funding_timestamp  funding_rate  predicted_funding_rate  \\\n",
      "0     1.577840e+15       1.577866e+15     -0.000161                     NaN   \n",
      "1     1.577844e+15       1.577866e+15     -0.000173                     NaN   \n",
      "2     1.577848e+15       1.577866e+15     -0.000183                     NaN   \n",
      "3     1.577851e+15       1.577866e+15     -0.000191                     NaN   \n",
      "4     1.577855e+15       1.577866e+15     -0.000187                     NaN   \n",
      "\n",
      "   open_interest  last_price   index_price   mark_price  \\\n",
      "0      26588.769     7171.55  11682.594955  7176.918847   \n",
      "1      26588.769     7208.32  11682.594955  7213.227865   \n",
      "2      26588.769     7237.49  11682.594955  7239.762514   \n",
      "3      26588.769     7221.55  11682.594955  7224.738329   \n",
      "4      26588.769     7214.00  11682.594955  7219.013281   \n",
      "\n",
      "                  timestamp  \n",
      "0 2020-01-01 00:00:00+00:00  \n",
      "1 2020-01-01 01:00:00+00:00  \n",
      "2 2020-01-01 02:00:00+00:00  \n",
      "3 2020-01-01 03:00:00+00:00  \n",
      "4 2020-01-01 04:00:00+00:00  \n",
      "Rescaling 'funding_rate'...\n",
      "Rescaling 'funding_rate' completed. Shape: (42179, 9)\n",
      "   local_timestamp  funding_timestamp  funding_rate  predicted_funding_rate  \\\n",
      "0     1.577840e+15       1.577866e+15       -161.14                     NaN   \n",
      "1     1.577844e+15       1.577866e+15       -173.37                     NaN   \n",
      "2     1.577848e+15       1.577866e+15       -182.86                     NaN   \n",
      "3     1.577851e+15       1.577866e+15       -191.27                     NaN   \n",
      "4     1.577855e+15       1.577866e+15       -186.62                     NaN   \n",
      "\n",
      "   open_interest  last_price   index_price   mark_price  \\\n",
      "0      26588.769     7171.55  11682.594955  7176.918847   \n",
      "1      26588.769     7208.32  11682.594955  7213.227865   \n",
      "2      26588.769     7237.49  11682.594955  7239.762514   \n",
      "3      26588.769     7221.55  11682.594955  7224.738329   \n",
      "4      26588.769     7214.00  11682.594955  7219.013281   \n",
      "\n",
      "                  timestamp  \n",
      "0 2020-01-01 00:00:00+00:00  \n",
      "1 2020-01-01 01:00:00+00:00  \n",
      "2 2020-01-01 02:00:00+00:00  \n",
      "3 2020-01-01 03:00:00+00:00  \n",
      "4 2020-01-01 04:00:00+00:00  \n",
      "Removing outliers from 'funding_rate'...\n",
      "Removing outliers from 'funding_rate' completed. Shape: (42179, 9)\n",
      "   local_timestamp  funding_timestamp  funding_rate  predicted_funding_rate  \\\n",
      "0     1.577840e+15       1.577866e+15       -161.14                     NaN   \n",
      "1     1.577844e+15       1.577866e+15       -173.37                     NaN   \n",
      "2     1.577848e+15       1.577866e+15       -182.86                     NaN   \n",
      "3     1.577851e+15       1.577866e+15       -191.27                     NaN   \n",
      "4     1.577855e+15       1.577866e+15       -186.62                     NaN   \n",
      "\n",
      "   open_interest  last_price   index_price   mark_price  \\\n",
      "0      26588.769     7171.55  11682.594955  7176.918847   \n",
      "1      26588.769     7208.32  11682.594955  7213.227865   \n",
      "2      26588.769     7237.49  11682.594955  7239.762514   \n",
      "3      26588.769     7221.55  11682.594955  7224.738329   \n",
      "4      26588.769     7214.00  11682.594955  7219.013281   \n",
      "\n",
      "                  timestamp  \n",
      "0 2020-01-01 00:00:00+00:00  \n",
      "1 2020-01-01 01:00:00+00:00  \n",
      "2 2020-01-01 02:00:00+00:00  \n",
      "3 2020-01-01 03:00:00+00:00  \n",
      "4 2020-01-01 04:00:00+00:00  \n",
      "Adding lag features...\n",
      "Adding lag features completed. Shape: (42179, 13)\n",
      "   local_timestamp  funding_timestamp  funding_rate  predicted_funding_rate  \\\n",
      "0     1.577840e+15       1.577866e+15       -161.14                     0.0   \n",
      "1     1.577844e+15       1.577866e+15       -173.37                     0.0   \n",
      "2     1.577848e+15       1.577866e+15       -182.86                     0.0   \n",
      "3     1.577851e+15       1.577866e+15       -191.27                     0.0   \n",
      "4     1.577855e+15       1.577866e+15       -186.62                     0.0   \n",
      "\n",
      "   open_interest  last_price   index_price   mark_price  \\\n",
      "0      26588.769     7171.55  11682.594955  7176.918847   \n",
      "1      26588.769     7208.32  11682.594955  7213.227865   \n",
      "2      26588.769     7237.49  11682.594955  7239.762514   \n",
      "3      26588.769     7221.55  11682.594955  7224.738329   \n",
      "4      26588.769     7214.00  11682.594955  7219.013281   \n",
      "\n",
      "                  timestamp  funding_rate_lag1  funding_rate_lag2  \\\n",
      "0 2020-01-01 00:00:00+00:00               0.00               0.00   \n",
      "1 2020-01-01 01:00:00+00:00            -161.14               0.00   \n",
      "2 2020-01-01 02:00:00+00:00            -173.37            -161.14   \n",
      "3 2020-01-01 03:00:00+00:00            -182.86            -173.37   \n",
      "4 2020-01-01 04:00:00+00:00            -191.27            -182.86   \n",
      "\n",
      "   open_interest_lag1  mark_price_lag1  \n",
      "0               0.000         0.000000  \n",
      "1           26588.769      7176.918847  \n",
      "2           26588.769      7213.227865  \n",
      "3           26588.769      7239.762514  \n",
      "4           26588.769      7224.738329  \n",
      "Adding technical indicators...\n",
      "Adding technical indicators completed. Shape: (42179, 21)\n",
      "   local_timestamp  funding_timestamp  funding_rate  predicted_funding_rate  \\\n",
      "0     1.577840e+15       1.577866e+15       -161.14                     0.0   \n",
      "1     1.577844e+15       1.577866e+15       -173.37                     0.0   \n",
      "2     1.577848e+15       1.577866e+15       -182.86                     0.0   \n",
      "3     1.577851e+15       1.577866e+15       -191.27                     0.0   \n",
      "4     1.577855e+15       1.577866e+15       -186.62                     0.0   \n",
      "\n",
      "   open_interest  last_price   index_price   mark_price  \\\n",
      "0      26588.769     7171.55  11682.594955  7176.918847   \n",
      "1      26588.769     7208.32  11682.594955  7213.227865   \n",
      "2      26588.769     7237.49  11682.594955  7239.762514   \n",
      "3      26588.769     7221.55  11682.594955  7224.738329   \n",
      "4      26588.769     7214.00  11682.594955  7219.013281   \n",
      "\n",
      "                  timestamp  funding_rate_lag1  ...  open_interest_lag1  \\\n",
      "0 2020-01-01 00:00:00+00:00               0.00  ...               0.000   \n",
      "1 2020-01-01 01:00:00+00:00            -161.14  ...           26588.769   \n",
      "2 2020-01-01 02:00:00+00:00            -173.37  ...           26588.769   \n",
      "3 2020-01-01 03:00:00+00:00            -182.86  ...           26588.769   \n",
      "4 2020-01-01 04:00:00+00:00            -191.27  ...           26588.769   \n",
      "\n",
      "   mark_price_lag1  funding_rate_ma3  funding_rate_ma5  funding_rate_ema3  \\\n",
      "0         0.000000               NaN               NaN        -161.140000   \n",
      "1      7176.918847               NaN               NaN        -167.255000   \n",
      "2      7213.227865       -172.456667               NaN        -175.057500   \n",
      "3      7239.762514       -182.500000               NaN        -183.163750   \n",
      "4      7224.738329       -186.916667          -179.052        -184.891875   \n",
      "\n",
      "   funding_rate_ema5  volatility_5h  funding_rate_roc1  funding_rate_roc3  \\\n",
      "0        -161.140000            NaN                NaN                NaN   \n",
      "1        -165.216667            NaN           0.075897                NaN   \n",
      "2        -171.097778            NaN           0.054738                NaN   \n",
      "3        -177.821852            NaN           0.045991           0.186980   \n",
      "4        -180.754568      23.329593          -0.024311           0.076426   \n",
      "\n",
      "   open_interest_roc  \n",
      "0                NaN  \n",
      "1                0.0  \n",
      "2                0.0  \n",
      "3                0.0  \n",
      "4                0.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Adding interaction terms...\n",
      "Adding interaction terms completed. Shape: (42179, 24)\n",
      "   local_timestamp  funding_timestamp  funding_rate  predicted_funding_rate  \\\n",
      "0     1.577840e+15       1.577866e+15       -161.14                     0.0   \n",
      "1     1.577844e+15       1.577866e+15       -173.37                     0.0   \n",
      "2     1.577848e+15       1.577866e+15       -182.86                     0.0   \n",
      "3     1.577851e+15       1.577866e+15       -191.27                     0.0   \n",
      "4     1.577855e+15       1.577866e+15       -186.62                     0.0   \n",
      "\n",
      "   open_interest  last_price   index_price   mark_price  \\\n",
      "0      26588.769     7171.55  11682.594955  7176.918847   \n",
      "1      26588.769     7208.32  11682.594955  7213.227865   \n",
      "2      26588.769     7237.49  11682.594955  7239.762514   \n",
      "3      26588.769     7221.55  11682.594955  7224.738329   \n",
      "4      26588.769     7214.00  11682.594955  7219.013281   \n",
      "\n",
      "                  timestamp  funding_rate_lag1  ...  funding_rate_ma5  \\\n",
      "0 2020-01-01 00:00:00+00:00               0.00  ...               NaN   \n",
      "1 2020-01-01 01:00:00+00:00            -161.14  ...               NaN   \n",
      "2 2020-01-01 02:00:00+00:00            -173.37  ...               NaN   \n",
      "3 2020-01-01 03:00:00+00:00            -182.86  ...               NaN   \n",
      "4 2020-01-01 04:00:00+00:00            -191.27  ...          -179.052   \n",
      "\n",
      "   funding_rate_ema3  funding_rate_ema5  volatility_5h  funding_rate_roc1  \\\n",
      "0        -161.140000        -161.140000            NaN                NaN   \n",
      "1        -167.255000        -165.216667            NaN           0.075897   \n",
      "2        -175.057500        -171.097778            NaN           0.054738   \n",
      "3        -183.163750        -177.821852            NaN           0.045991   \n",
      "4        -184.891875        -180.754568      23.329593          -0.024311   \n",
      "\n",
      "   funding_rate_roc3  open_interest_roc  interaction1  interaction2  \\\n",
      "0                NaN                NaN        0.0000      0.000000   \n",
      "1                NaN                0.0       -0.0000      0.000000   \n",
      "2                NaN                0.0    27936.8418      0.994732   \n",
      "3           0.186980                0.0    31702.4382      0.998031   \n",
      "4           0.076426                0.0    34975.6322      0.977240   \n",
      "\n",
      "   interaction3  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2 -1.243969e+06  \n",
      "3 -1.321257e+06  \n",
      "4 -1.350424e+06  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Creating features and target variable...\n",
      "Error during creating features and target variable: create_features() got an unexpected keyword argument 'keep_future_rate'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 80\u001b[0m\n\u001b[0;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m process_pipeline(\n\u001b[0;32m     72\u001b[0m     BINANCE_BTC_PERP_CSV,\n\u001b[0;32m     73\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     keep_future_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Set index\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_index'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SKIP_CELL = True\n",
    "\n",
    "# absolute path of the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# project root directory to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import data processing functions\n",
    "from utilities.data_processing import (\n",
    "    load_data, \n",
    "    preprocess_data, \n",
    "    create_features, \n",
    "    process_pipeline,\n",
    ")\n",
    "\n",
    "# Import feature engineering and data balancing utilities\n",
    "from utilities.functions import (\n",
    "    add_lag_features,\n",
    "    add_technical_indicators,\n",
    "    add_interaction_terms,\n",
    "    add_model1_direction,\n",
    "    add_model2_volatility,\n",
    "    plot_acf_pacf,\n",
    "    perform_ljung_box_test,\n",
    "    remove_outliers,\n",
    "    rescale_series,\n",
    "    winsorize_series\n",
    "    )\n",
    "\n",
    "# Import model-related utilities\n",
    "from utilities.model_utils import (\n",
    "    train_classification_model,\n",
    "    evaluate_classification_model,\n",
    "    save_model,\n",
    "    load_model,\n",
    "    save_garch_model,\n",
    "    load_garch_model,\n",
    "    perform_hyperparameter_tuning,\n",
    "    plot_feature_importance,\n",
    "    fit_and_evaluate_model\n",
    ")\n",
    "\n",
    "# Import configurations\n",
    "from config import (\n",
    "    BINANCE_BTC_PERP_CSV,\n",
    "    PREDICTIONS_RFR_CSV,\n",
    "    PREDICTIONS_SARIMAX_CSV,\n",
    "    RANDOM_STATE,\n",
    "    MODEL1_RF_PATH,\n",
    "    SCALER1_RF_PATH,\n",
    "    MODEL2_GARCH_PATH,\n",
    "    MODEL3_RFR_PATH,\n",
    "    MODEL3_SARIMAX_PATH\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Call process_pipeline only once to get the fully processed df\n",
    "df = process_pipeline(\n",
    "    BINANCE_BTC_PERP_CSV,\n",
    "    rescale=True,\n",
    "    scaling_factor=1e6,\n",
    "    handle_outliers=True,\n",
    "    keep_future_rate=True\n",
    ")\n",
    "\n",
    "# Set index\n",
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Data Preprocessing and Feature Engineering**\n",
    "\n",
    "In this step, we prepare the data for modeling by performing preprocessing tasks and creating new features that may improve model performance.\n",
    "\n",
    "### **1.1 Determining Funding Rate Direction**\n",
    "\n",
    "We create the target variable `direction` to indicate whether the funding rate is expected to **increase (`1`)** or **decrease/remain the same (`0`)** in the next time period.\n",
    "\n",
    "- **Methodology**:\n",
    "  - Shift the `funding_rate` column by one period to get the future funding rate.\n",
    "  - Compare the future funding rate with the current funding rate to determine the direction.\n",
    "  - The `direction` is set to `1` if the future funding rate is higher; otherwise, it's `0`.\n",
    "\n",
    "### **1.2 Feature Engineering**\n",
    "\n",
    "To enhance the model's predictive power, we generate additional features:\n",
    "\n",
    "- **Lag Features**:\n",
    "  - `funding_rate_lag1`: Funding rate from the previous period.\n",
    "  - `open_interest_lag1`: Open interest from the previous period.\n",
    "  - `mark_price_lag1`: Mark price from the previous period.\n",
    "\n",
    "- **Technical Indicators**:\n",
    "  - `funding_rate_ma3`: 3-period moving average of the funding rate.\n",
    "\n",
    "- **Cyclical Time Features**:\n",
    "  - Convert time-based features (hour, day, month) into cyclical features using sine and cosine transformations to capture periodic patterns.\n",
    "\n",
    "- **Data Handling**:\n",
    "  - **Missing Values**: Filled `NaN` values resulting from lagging and moving averages using backward fill (`bfill`).\n",
    "  - **Scaling**: Standardized numerical features to ensure they're on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Random Forest Classifier**\n",
    "\n",
    "#### **Description**\n",
    "\n",
    "Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    try:\n",
    "        print(\"Data after pipeline processing:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Define features and target directly from the pipeline output\n",
    "        feature_columns = [\n",
    "            'funding_rate_lag1', 'funding_rate_lag2',\n",
    "            'funding_rate_ma3', 'funding_rate_ma5', 'funding_rate_ema3',\n",
    "            'open_interest', 'open_interest_lag1', 'open_interest_roc',\n",
    "            'mark_price', 'mark_price_lag1', 'volatility_5min',\n",
    "            'funding_rate_roc1', 'funding_rate_roc3', 'interaction2', 'interaction3'\n",
    "        ]\n",
    "\n",
    "        X = df[feature_columns]\n",
    "        y = df['direction']\n",
    "\n",
    "        print(\"Splitting the data...\")\n",
    "        split_index = int(0.8 * len(X))\n",
    "        X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "        y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "        if not X_train.empty and not X_test.empty:\n",
    "            print(\"Starting preprocessing for Random Forest...\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Handle NaNs and infinities (just in case)\n",
    "            X_train_rf = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            X_test_rf = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "            # Fit a StandardScaler on training data and transform\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train_rf)\n",
    "            X_train_scaled = scaler.transform(X_train_rf)\n",
    "            X_test_scaled = scaler.transform(X_test_rf)\n",
    "\n",
    "            print(f\"Preprocessing for Random Forest completed in {time.time() - start_time:.2f} seconds.\")\n",
    "            print(f\"Training data shape for Random Forest: {X_train_scaled.shape}\")\n",
    "            print(f\"Test data shape for Random Forest: {X_test_scaled.shape}\")\n",
    "\n",
    "            # Train initial Random Forest model\n",
    "            print(\"Training initial Random Forest model...\")\n",
    "            start_time = time.time()\n",
    "            rf_model = RandomForestClassifier(n_estimators=50, random_state=42, class_weight='balanced')\n",
    "            rf_model.fit(X_train_scaled, y_train)\n",
    "            print(f\"Random Forest training completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "            print(\"\\nInitial Model Evaluation:\")\n",
    "            evaluate_classification_model(rf_model, X_test_scaled, y_test)\n",
    "\n",
    "            # Start hyperparameter tuning\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            start_time = time.time()\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100],\n",
    "                'max_depth': [10, 20],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "            best_rf_model = perform_hyperparameter_tuning(\n",
    "                rf_model,\n",
    "                param_grid,\n",
    "                X_train_scaled,\n",
    "                y_train,\n",
    "                scoring='roc_auc',\n",
    "                verbose=1\n",
    "            )\n",
    "            print(f\"Hyperparameter tuning completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "            print(\"\\nBest Model Evaluation After Hyperparameter Tuning:\")\n",
    "            evaluate_classification_model(best_rf_model, X_test_scaled, y_test)\n",
    "\n",
    "            # Evaluate with a custom threshold\n",
    "            y_proba = best_rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "            custom_threshold = 0.5\n",
    "            print(f\"\\nEvaluation with Custom Threshold ({custom_threshold}):\")\n",
    "            evaluate_classification_model(best_rf_model, X_test_scaled, y_test, y_proba, threshold=custom_threshold)\n",
    "\n",
    "            # Plot feature importance\n",
    "            print(\"Plotting feature importance...\")\n",
    "            plot_feature_importance(best_rf_model, X_train_rf.columns)\n",
    "\n",
    "            # Save the model and scaler\n",
    "            print(\"Saving the model and scaler...\")\n",
    "            save_model(best_rf_model, '../models/saved_models/model1_RF.pkl')\n",
    "            save_model(scaler, '../models/saved_models/scaler1_RF.pkl')\n",
    "            print(\"Model and scaler saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Random Forest training and tuning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Logistic Regression**\n",
    "\n",
    "#### **Description**\n",
    "\n",
    "Logistic Regression is a linear model commonly used for binary classification problems. It models the probability that a given input belongs to a particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_CELL:\n",
    "    # Imports\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Check for dataset existence\n",
    "    if not os.path.exists(BINANCE_BTC_PERP_CSV):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {BINANCE_BTC_PERP_CSV}\")\n",
    "    else:\n",
    "        print(f\"Dataset found at {BINANCE_BTC_PERP_CSV}\")\n",
    "\n",
    "    # Start pipeline execution\n",
    "    print(\"Starting pipeline execution...\")\n",
    "    df = process_pipeline(BINANCE_BTC_PERP_CSV)\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(\"Dataset not processed properly. Check pipeline.\")\n",
    "        raise ValueError(\"Processed dataset is empty. Check the pipeline.\")\n",
    "\n",
    "    print(f\"Shape of DataFrame after processing: {df.shape}\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Splitting the data\n",
    "    print(\"Splitting the data...\")\n",
    "    feature_columns = [\n",
    "        'funding_rate_lag1', 'funding_rate_lag2',\n",
    "        'funding_rate_ma5', 'funding_rate_ema3',\n",
    "        'mark_price', 'mark_price_lag1',\n",
    "        'funding_rate_ma3'\n",
    "    ]\n",
    "\n",
    "    missing_columns = [col for col in feature_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns: {missing_columns}\")\n",
    "        raise ValueError(f\"The following required columns are missing: {missing_columns}\")\n",
    "\n",
    "    X = df[feature_columns]\n",
    "    y = df['direction']\n",
    "\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    print(f\"Target variable: {y.name}\")\n",
    "\n",
    "    split_index = int(0.8 * len(X))\n",
    "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "    # Preprocessing\n",
    "    if not X_train.empty and not X_test.empty:\n",
    "        print(\"Starting preprocessing...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use preprocess_data from data_processing.py\n",
    "        X_train = preprocess_data(X_train, handle_timestamps=False)\n",
    "        X_test = preprocess_data(X_test, handle_timestamps=False)\n",
    "\n",
    "        numeric_features = [\n",
    "            'funding_rate_lag1', 'funding_rate_lag2',\n",
    "            'funding_rate_ma5', 'funding_rate_ema3',\n",
    "            'mark_price', 'mark_price_lag1',\n",
    "            'funding_rate_ma3'\n",
    "        ]\n",
    "\n",
    "        # Debug preview\n",
    "        print(\"Preview of X_train before scaling:\")\n",
    "        print(X_train[numeric_features].head())\n",
    "\n",
    "        # Scaling features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train[numeric_features])\n",
    "        X_test_scaled = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "        # Prepare processed data\n",
    "        X_train_prepared = pd.DataFrame(X_train_scaled, columns=numeric_features, index=X_train.index)\n",
    "        X_test_prepared = pd.DataFrame(X_test_scaled, columns=numeric_features, index=X_test.index)\n",
    "\n",
    "        print(f\"Preprocessing completed in {time.time() - start_time:.2f} seconds.\")\n",
    "        print(f\"Training data shape after preprocessing: {X_train_prepared.shape}\")\n",
    "        print(f\"Test data shape after preprocessing: {X_test_prepared.shape}\")\n",
    "\n",
    "        # Train Logistic Regression model\n",
    "        print(\"Training Logistic Regression model...\")\n",
    "        start_time = time.time()\n",
    "        logreg_model = LogisticRegression(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            solver='liblinear'\n",
    "        )\n",
    "        logreg_model.fit(X_train_prepared, y_train)\n",
    "        print(f\"Logistic Regression training completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "        # Evaluate initial model\n",
    "        print(\"\\nInitial Logistic Regression Model Evaluation:\")\n",
    "        evaluate_classification_model(logreg_model, X_test_prepared, y_test)\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        start_time = time.time()\n",
    "        param_grid_logreg = {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "        best_logreg_model = perform_hyperparameter_tuning(\n",
    "            LogisticRegression(\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            param_grid_logreg,\n",
    "            X_train_prepared,\n",
    "            y_train,\n",
    "            scoring='f1'\n",
    "        )\n",
    "        print(f\"Hyperparameter tuning completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "        # Evaluate best model\n",
    "        print(\"\\nBest Logistic Regression Model Evaluation After Hyperparameter Tuning:\")\n",
    "        evaluate_classification_model(best_logreg_model, X_test_prepared, y_test)\n",
    "\n",
    "        # Adjust classification threshold\n",
    "        y_proba_logreg = best_logreg_model.predict_proba(X_test_prepared)[:, 1]\n",
    "        custom_threshold = 0.4\n",
    "        print(f\"\\nEvaluation with Custom Threshold ({custom_threshold}):\")\n",
    "        evaluate_classification_model(best_logreg_model, X_test_prepared, y_test, y_proba_logreg, threshold=custom_threshold)\n",
    "\n",
    "        # Plot feature importance\n",
    "        print(\"Plotting feature importance...\")\n",
    "        plot_feature_importance(best_logreg_model, X_train_prepared.columns, model_type='logistic_regression')\n",
    "\n",
    "        # Save the Logistic Regression model\n",
    "        print(\"Saving the Logistic Regression model...\")\n",
    "        save_model(best_logreg_model, '../models/saved_models/model1_LR.pkl')\n",
    "        save_model(scaler, '../models/saved_models/scaler1_LR.pkl')\n",
    "\n",
    "    else:\n",
    "        print(\"Training or test datasets are empty. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Model Comparison**\n",
    "\n",
    "- Compared the performance of logistic regression and Random Forest models.\n",
    "- Discussed which model performed better and why.\n",
    "- Considered factors like overfitting, computational efficiency, and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n",
    "\n",
    "- **Initial Accuracy**: 75.79%\n",
    "- **After Tuning Accuracy**: 78.75% (+3%)\n",
    "- **F1 Score After Tuning**: 71.27% (+20%)\n",
    "- **Strength**: Captures non-linear interactions effectively.\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "- **Accuracy**: 80.26% (unchanged post-tuning)\n",
    "- **F1 Score**: 65.89%\n",
    "- **Limitation**: Struggles with non-linear relationships.\n",
    "\n",
    "#### Best Model: Random Forest Classifier\n",
    "\n",
    "- **Reason**: Higher recall and F1 Score for imbalanced data, making it better suited for this dataset.\n",
    "\n",
    "#### Improvement Suggestions\n",
    "\n",
    "1. Add polynomial features or interactions for better representation of the data.\n",
    "2. Use **SMOTE** to handle the imbalanced dataset effectively.\n",
    "3. Experiment with ensemble models like stacking or boosting for improved performance.\n",
    "4. Test **Elastic Net** regularization to optimize Logistic Regression.\n",
    "5. Apply stratified K-fold cross-validation to evaluate model robustness.\n",
    "\n",
    "TRY XGBOOST AND LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL2 - Predicting Funding Rate Volatility**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Building on **Model 1** (Random Forest Classifier), which predicted the funding rate's direction, **Model 2** focuses on forecasting **volatility** to quantify the magnitude of market fluctuations. Accurate volatility predictions are critical for assessing risk and market movement intensity.\n",
    "\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "- Predict the **volatility** (variance) of funding rates to evaluate the extent of potential market fluctuations.\n",
    "\n",
    "\n",
    "### **Segway from Model 1**\n",
    "\n",
    "- **Input**: Direction predictions (up/down) from **Model 1** serve as a guiding feature to contextualize volatility estimation.\n",
    "\n",
    "\n",
    "### **Approach**\n",
    "\n",
    "- Implement and compare **GARCH-based models**:\n",
    "  - **GARCH (1,1)**: Captures volatility clustering.\n",
    "  - **EGARCH**: Models asymmetric shocks.\n",
    "  - **GJR-GARCH**: Accounts for leverage effects.\n",
    "- Evaluate models using statistical metrics like **AIC** and **BIC**.\n",
    "\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "- Forecast volatility using the best GARCH model.\n",
    "- Save and integrate the output with **Model 3** to predict exact funding rate values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# ===========================================\n",
    "# Data Preprocessing and Feature Engineering\n",
    "# ===========================================\n",
    "\n",
    "# Process the data using the pipeline\n",
    "df = process_pipeline(BINANCE_BTC_PERP_CSV)\n",
    "\n",
    "# Ensure 'timestamp' is set as index for time series analysis\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Extract the funding rate series\n",
    "funding_rate_series = df['funding_rate']\n",
    "\n",
    "# Handle missing values in 'funding_rate'\n",
    "funding_rate_series = funding_rate_series.dropna()\n",
    "\n",
    "# ===========================================\n",
    "# Stationarity Check\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\nPerforming Augmented Dickey-Fuller (ADF) Test for Stationarity...\\n\")\n",
    "adf_result = adfuller(funding_rate_series)\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "\n",
    "if adf_result[1] > 0.05:\n",
    "    print(\"The series is non-stationary. Differencing is required.\")\n",
    "    funding_rate_series = funding_rate_series.diff().dropna()\n",
    "    print(\"Performed first differencing to achieve stationarity.\")\n",
    "else:\n",
    "    print(\"The series is stationary.\")\n",
    "    \n",
    "# ===========================================\n",
    "# Plot ACF and PACF\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\nPlotting ACF and PACF...\\n\")\n",
    "plot_acf_pacf(funding_rate_series, lags=50)\n",
    "\n",
    "# ===========================================\n",
    "# Fit GARCH Models\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\nFitting GARCH Models...\\n\")\n",
    "\n",
    "# Define models to fit\n",
    "models_to_fit = [\n",
    "    (\"GARCH(1,1) Normal\", arch_model(funding_rate_series, vol='GARCH', p=1, q=1, mean='Zero', dist='normal')),\n",
    "    (\"GARCH(1,1) t\", arch_model(funding_rate_series, vol='GARCH', p=1, q=1, mean='Zero', dist='t')),\n",
    "    (\"EGARCH(1,1) Normal\", arch_model(funding_rate_series, vol='EGARCH', p=1, q=1, mean='Zero', dist='normal')),\n",
    "    (\"GJR-GARCH(1,1) Normal\", arch_model(funding_rate_series, vol='GARCH', p=1, o=1, q=1, mean='Zero', dist='normal')),\n",
    "    (\"GJR-GARCH(1,1) AR(1)\", arch_model(funding_rate_series, mean='AR', lags=1, vol='GARCH', p=1, o=1, q=1, dist='normal')),\n",
    "    (\"GJR-GARCH(1,1) GED\", arch_model(funding_rate_series, mean='Zero', vol='GARCH', p=1, o=1, q=1, dist='ged')),\n",
    "    (\"GJR-GARCH(1,1) AR(2)\", arch_model(funding_rate_series, mean='AR', lags=2, vol='GARCH', p=1, o=1, q=1, dist='normal')),\n",
    "    (\"GJR-GARCH(2,1)\", arch_model(funding_rate_series, mean='Zero', vol='GARCH', p=2, o=1, q=1, dist='normal')),\n",
    "    (\"EGARCH(1,1) t\", arch_model(funding_rate_series, vol='EGARCH', p=1, q=1, mean='Zero', dist='t')),\n",
    "]\n",
    "    \n",
    "# Fit models and store results\n",
    "models_results = {}\n",
    "for name, model in models_to_fit:\n",
    "    result = fit_and_evaluate_model(name, model)\n",
    "    if result is not None:\n",
    "        models_results[name] = result\n",
    "\n",
    "# Check if any models were successfully fitted\n",
    "if not models_results:\n",
    "    print(\"Error: No models were successfully fitted. Check the data or model configurations.\")\n",
    "    raise ValueError(\"No valid models in models_results.\")\n",
    "\n",
    "# ===========================================\n",
    "# Model Selection and Evaluation\n",
    "# ===========================================\n",
    "\n",
    "# Comparing Models Using AIC and BIC\n",
    "print(\"\\nComparing Models Using AIC and BIC...\\n\")\n",
    "for name, result in models_results.items():\n",
    "    print(f\"{name} - AIC: {result.aic:.3f}, BIC: {result.bic:.3f}\")\n",
    "\n",
    "\n",
    "# Select the best model based on AIC\n",
    "best_model_name = min(models_results, key=lambda k: models_results[k].aic)\n",
    "best_model_result = models_results[best_model_name]\n",
    "print(f\"\\nSelected Model Based on Lowest AIC: {best_model_name}\\n\")\n",
    "\n",
    "# ===========================================\n",
    "# Feature Importance Plot\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\nPlotting Feature Importance...\\n\")\n",
    "\n",
    "# Extract parameter names and values from the GARCH model\n",
    "feature_importance_df = best_model_result.params.abs().sort_values(ascending=False).reset_index()\n",
    "feature_importance_df.columns = ['Feature', 'Importance']\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'], color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importance (GARCH Parameters)')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===========================================\n",
    "# Residual Analysis\n",
    "# ===========================================\n",
    "\n",
    "print(\"Analyzing Residuals...\\n\")\n",
    "\n",
    "# Get standardized residuals\n",
    "residuals = best_model_result.std_resid\n",
    "\n",
    "# Perform Ljung-Box test on residuals\n",
    "print(\"Performing Ljung-Box Test on Residuals:\")\n",
    "perform_ljung_box_test(residuals, lags=10)\n",
    "\n",
    "# Perform Ljung-Box test on squared residuals\n",
    "print(\"\\nPerforming Ljung-Box Test on Squared Residuals:\")\n",
    "perform_ljung_box_test(residuals**2, lags=10)\n",
    "\n",
    "# Plot standardized residuals\n",
    "print(\"\\nPlotting Standardized Residuals...\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(residuals)\n",
    "plt.title('Standardized Residuals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Standardized Residual')\n",
    "plt.show()\n",
    "\n",
    "# Plot conditional volatility\n",
    "print(\"Plotting Conditional Volatility...\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(best_model_result.conditional_volatility)\n",
    "plt.title('Conditional Volatility')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volatility')\n",
    "plt.show()\n",
    "\n",
    "# ===========================================\n",
    "# Forecasting\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\nForecasting Future Volatility...\\n\")\n",
    "\n",
    "# Forecast volatility for the next 5 periods\n",
    "forecast_horizon = 5\n",
    "volatility_forecast = best_model_result.forecast(horizon=forecast_horizon)\n",
    "\n",
    "# Extract forecasted conditional variances\n",
    "forecasted_variances = volatility_forecast.variance.iloc[-1]\n",
    "\n",
    "print(\"Forecasted Conditional Variances:\")\n",
    "print(forecasted_variances)\n",
    "\n",
    "# Plot forecasted volatility\n",
    "print(\"\\nPlotting Forecasted Volatility...\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.sqrt(forecasted_variances.values), marker='o')\n",
    "plt.title(f'Forecasted Volatility for Next {forecast_horizon} Periods')\n",
    "plt.xlabel('Steps Ahead')\n",
    "plt.ylabel('Volatility')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===========================================\n",
    "# Saving the Model\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\nSaving the Best GARCH Model...\\n\")\n",
    "\n",
    "# Save the best GARCH model\n",
    "save_garch_model(best_model_result, MODEL2_GARCH_PATH)\n",
    "print(f\"GARCH model saved to {MODEL2_GARCH_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detailed Analysis of Results**\n",
    "\n",
    "#### **1. Model Selection Results:**\n",
    "- The **`GJR-GARCH(1,1) AR(2)`** model has the lowest AIC (-732755.679) and BIC (-732695.682), indicating it is the best fit for the data, balancing model accuracy and complexity.\n",
    "- AR(2) terms effectively capture dependencies in the funding rate, supported by PACF spikes at lags 1 and 2.\n",
    "\n",
    "#### **2. Ljung-Box Test Results on Residuals:**\n",
    "- Ljung-Box test results return NaN, suggesting potential data issues; this does not invalidate the model but highlights the need for additional diagnostic checks.\n",
    "\n",
    "#### **3. Residual Analysis (Standardized Residuals):**\n",
    "- Residuals are stationary with occasional spikes, reflecting periods of market stress. This confirms the model captures volatility clustering and explains most data variability.\n",
    "- High residual spikes during specific periods may indicate external market-driven anomalies not fully modeled.\n",
    "\n",
    "#### **4. Conditional Volatility:**\n",
    "- Conditional volatility shows clusters of market stress followed by periods of stabilization, demonstrating the model's ability to reflect time-varying market risks.\n",
    "- Peaks in volatility confirm the model's reliability in identifying high-risk and stable phases in the market.\n",
    "\n",
    "#### **5. Forecasted Volatility:**\n",
    "- Forecasted variances increase steadily over the next five steps:\n",
    "  ```\n",
    "  h.1: 1.609353e-10\n",
    "  h.2: 3.964265e-10\n",
    "  h.3: 6.709361e-10\n",
    "  h.4: 9.769573e-10\n",
    "  h.5: 1.311498e-09\n",
    "  ```\n",
    "  Indicating rising uncertainty and potential market instability in the near term.\n",
    "\n",
    "### **Plot-Specific Observations**\n",
    "\n",
    "#### **1. ACF and PACF Plots:**\n",
    "- ACF’s slow decay and PACF’s sharp spikes at lags 1 and 2 confirm the relevance of AR(2) terms in the mean model, supporting the selected configuration.\n",
    "\n",
    "#### **2. Standardized Residuals Plot:**\n",
    "- Residual spikes during market stress align with large movements, confirming the model handles volatility well without leaving patterns in residuals.\n",
    "\n",
    "#### **3. Conditional Volatility Plot:**\n",
    "- Volatility peaks during market stress and declines during stability, validating the model’s ability to capture time-varying risk.\n",
    "\n",
    "#### **4. Forecasted Volatility Plot:**\n",
    "- Increasing forecasted volatility indicates rising market risk over the forecast horizon, aiding in risk preparation and strategy adjustment.\n",
    "\n",
    "### **Summary of Findings**\n",
    "- The **`GJR-GARCH(1,1) AR(2)`** model is optimal for analyzing and forecasting funding rate volatility, successfully identifying risk patterns and forecasting future instability.\n",
    "- While residual diagnostics (e.g., Ljung-Box results) may require further exploration, the model demonstrates strong predictive capabilities for managing market risk and informing trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL3 - RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract funding_rate_series and check stationarity\n",
    "funding_rate_series = df['funding_rate'].dropna()\n",
    "adf_result = adfuller(funding_rate_series)\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "if adf_result[1] > 0.05:\n",
    "    print(\"Series non-stationary, differencing if needed.\")\n",
    "    funding_rate_series = funding_rate_series.diff().dropna()\n",
    "\n",
    "# Plot ACF/PACF if needed\n",
    "plot_acf_pacf(funding_rate_series, lags=50)\n",
    "\n",
    "# Add Model 1 direction predictions\n",
    "df = add_model1_direction(df)\n",
    "\n",
    "# Add Model 2 volatility forecast\n",
    "df = add_model2_volatility(df, steps=5)\n",
    "\n",
    "# For Model 3, we aim to predict `future_funding_rate` (one step ahead)\n",
    "df['future_funding_rate'] = df['funding_rate'].shift(-1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Features for Model 3:\n",
    "features = [c for c in df.columns if c not in ['funding_rate', 'future_funding_rate', 'direction']]\n",
    "X = df[features]\n",
    "y = df['future_funding_rate']\n",
    "\n",
    "# Split into training and test sets\n",
    "split_idx = int(0.8 * len(df))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Final cleanup before training Model 3\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_train = X_train.clip(-1e9, 1e9)\n",
    "X_train = X_train.astype(np.float64)\n",
    "\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test = X_test.clip(-1e9, 1e9)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "print(\"After cleanup:\")\n",
    "print(\"X_train describe:\\n\", X_train.describe())\n",
    "print(\"X_test describe:\\n\", X_test.describe())\n",
    "\n",
    "# Train a RandomForestRegressor for Model 3\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "m3_model = RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=100)\n",
    "m3_model.fit(X_train, y_train)\n",
    "print(\"\\nModel 3 training completed.\")\n",
    "\n",
    "# Evaluate Model 3\n",
    "y_pred = m3_model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\nModel 3 Evaluation:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Save Model 3\n",
    "save_model(m3_model, MODEL3_RFR_PATH)\n",
    "print(f\"Model 3 saved to {MODEL3_RFR_PATH}\")\n",
    "\n",
    "# Plot Feature Importance for Model 3\n",
    "plot_feature_importance(m3_model, X_train.columns, model_type='tree')\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Export predictions (Actual vs. Predicted) to CSV\n",
    "# ------------------------------------------------------\n",
    "results_df = X_test.copy()\n",
    "results_df['Actual'] = y_test\n",
    "results_df['Predicted'] = y_pred\n",
    "\n",
    "# If 'timestamp' was the index, let's bring it back as a column\n",
    "results_df.reset_index(inplace=True)\n",
    "\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.to_csv(PREDICTIONS_RFR_CSV, index=False)\n",
    "print(f\"Predictions and actual values saved to {PREDICTIONS_RFR_CSV}\")\n",
    "\n",
    "# Interpretation:\n",
    "# Model 3 uses model1_direction_pred and model2_volatility_h1 as additional features.\n",
    "# Further optimization: Try ARIMAX/SARIMAX or LSTM with direction & volatility as exogenous inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Inference\n",
    "\n",
    "**Stationarity (ADF Test):**  \n",
    "- **ADF Statistic: -8.9826**, **p-value: ~7.24e-15**: The series is strongly stationary or stationarized by differencing. This supports the use of ARIMA-type models.\n",
    "\n",
    "**ACF/PACF Interpretation:**  \n",
    "- **ACF:** High autocorrelation at lag 1 that decays slowly suggests persistent memory in the series.\n",
    "- **PACF:** Strong spike at lag 1 indicates an AR(1) or AR(2) structure may suffice. Minimal further significant lags suggest limited benefit from high-order AR terms.\n",
    "\n",
    "**Data Scaling and Cleanup:**  \n",
    "- Extreme values in features (e.g., `funding_rate_roc3`, `interaction3`) were clipped and filled, ensuring numerical stability.\n",
    "- After cleanup, no infinities/NaNs remain. Data is numeric and within manageable ranges.\n",
    "\n",
    "**Model Results (SARIMAX with `(2,1,1)`):**  \n",
    "- Achieved near-zero MSE and ~97.5% R².\n",
    "- Such exceptional metrics are rare in financial contexts, possibly due to:\n",
    "  - Effective capture of short-term patterns.\n",
    "  - The chosen scaling and feature engineering simplifying the prediction.\n",
    "  - Potential overfitting to the given train/test split.\n",
    "\n",
    "**Feature Importance (from previous tree-based model):**  \n",
    "- `funding_rate_ema3` dominated, indicating short-term historical averages drive predictions. This aligns with ARIMA models focusing on recent past values.\n",
    "\n",
    "**Future Prospects:**  \n",
    "- **Validation:** Use rolling cross-validation to confirm stability and generalization.\n",
    "- **Parameter Tuning:** Experiment with different `(p,d,q)` orders or introduce seasonal components `(P,D,Q,m)` if seasonal patterns exist.\n",
    "- **Exogenous Data:** Integrate more external features to reduce reliance on single dominant predictors.\n",
    "- **Multi-step Forecasting:** Test accuracy beyond one-step-ahead predictions to ensure robustness.\n",
    "\n",
    "In summary, the current SARIMAX model shows excellent performance on the given data. However, further validation, careful re-checking of scaling and feature sets, and exploration of alternative parameter sets are recommended to ensure these results are genuine and generalizable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL3 - SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "print(\"\\n--- Model 3 (SARIMAX) Improved Configuration ---\\n\")\n",
    "\n",
    "# Process data pipeline\n",
    "df = process_pipeline(\n",
    "    BINANCE_BTC_PERP_CSV,\n",
    "    rescale=True,\n",
    "    scaling_factor=1e6,\n",
    "    handle_outliers=True\n",
    ")\n",
    "\n",
    "# Use integer indexing (no datetime/frequency)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Extract funding rate series\n",
    "funding_rate_series = df['funding_rate'].dropna()\n",
    "\n",
    "# Stationarity Check\n",
    "adf_result = adfuller(funding_rate_series)\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "if adf_result[1] > 0.05:\n",
    "    print(\"Series is non-stationary. Applying first differencing...\")\n",
    "    funding_rate_series = funding_rate_series.diff().dropna()\n",
    "else:\n",
    "    print(\"Series is stationary.\")\n",
    "\n",
    "# Visualize ACF and PACF for the differenced series\n",
    "print(\"\\nPlotting ACF and PACF after differencing...\\n\")\n",
    "plot_acf_pacf(funding_rate_series, lags=50)\n",
    "\n",
    "# Add exogenous features from Model 1 and Model 2\n",
    "df = add_model1_direction(df)\n",
    "df = add_model2_volatility(df, steps=5)\n",
    "\n",
    "# Shift the target variable to create labels\n",
    "df['future_funding_rate'] = df['funding_rate'].shift(-1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "features = [c for c in df.columns if c not in ['funding_rate', 'future_funding_rate', 'direction']]\n",
    "X = df[features]\n",
    "y = df['future_funding_rate']\n",
    "\n",
    "# Train-test split\n",
    "split_idx = int(0.8 * len(df))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Preprocess data: Ensure numeric and handle infinities\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "X_train = X_train.replace([np.inf, -np.inf], 0.0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "X_train[numeric_columns] = X_train[numeric_columns].clip(-1e5, 1e5).astype(np.float64)\n",
    "X_test[numeric_columns] = X_test[numeric_columns].clip(-1e5, 1e5).astype(np.float64)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"After preprocessing:\")\n",
    "print(\"X_train describe:\\n\", X_train.describe())\n",
    "print(\"X_test describe:\\n\", X_test.describe())\n",
    "\n",
    "# SARIMAX Model Configuration\n",
    "order = (3, 1, 3)  # Updated configuration for better performance\n",
    "seasonal_order = (0, 0, 0, 0)  # No seasonal components\n",
    "\n",
    "print(\"\\nFitting SARIMAX model with improved configuration...\")\n",
    "sarimax_model = SARIMAX(\n",
    "    endog=y_train,\n",
    "    exog=X_train,\n",
    "    order=order,\n",
    "    seasonal_order=seasonal_order,\n",
    "    enforce_stationarity=True,\n",
    "    enforce_invertibility=True,\n",
    "    trend=None  # Adding constant trend\n",
    ")\n",
    "\n",
    "sarimax_result = sarimax_model.fit(disp=False, maxiter=500, method='powell')\n",
    "print(\"SARIMAX model training completed.\")\n",
    "\n",
    "# Predict using test set\n",
    "start = split_idx\n",
    "end = split_idx + len(y_test) - 1\n",
    "y_pred = sarimax_result.predict(start=start, end=end, exog=X_test)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Ensure no NaN or infinite values in predictions\n",
    "if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
    "    print(\"Predictions contain NaN or infinite values. Replacing with 0.0 for MSE/R² calculation.\")\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Calculate Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\nModel 3 (SARIMAX) Evaluation:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Residual Analysis\n",
    "print(\"\\nResidual Analysis...\")\n",
    "residuals = sarimax_result.resid\n",
    "standardized_residuals = residuals / np.std(residuals)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(standardized_residuals)\n",
    "plt.title('Standardized Residual Analysis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Standardized Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test for residuals\n",
    "print(\"\\nPerforming Ljung-Box test for residuals...\")\n",
    "lb_test = acorr_ljungbox(standardized_residuals, lags=[10], return_df=True)\n",
    "print(lb_test)\n",
    "\n",
    "# Save the model\n",
    "save_model(sarimax_result, MODEL3_SARIMAX_PATH)\n",
    "print(f\"SARIMAX model saved to {MODEL3_SARIMAX_PATH}\")\n",
    "\n",
    "# Export predictions and metrics\n",
    "results_df = X_test.copy()\n",
    "results_df['Actual'] = y_test\n",
    "results_df['Predicted'] = y_pred\n",
    "print(\"\\nPredictions and Metrics:\")\n",
    "print(results_df[['Actual', 'Predicted']].head())\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'R²'],\n",
    "    'Value': [mse, r2]\n",
    "})\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(PREDICTIONS_SARIMAX_CSV, index=False)\n",
    "print(f\"Predictions and actual values saved to {PREDICTIONS_SARIMAX_CSV}\")\n",
    "\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL vs PREDICTED for RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file paths\n",
    "csv_path = r\"C:\\Users\\viraj\\BuildSpace\\Projects\\stat429_project\\results\\predictions_RFR.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check if the necessary columns exist\n",
    "if 'Actual' not in df.columns or 'Predicted' not in df.columns:\n",
    "    raise ValueError(\"The CSV must have 'Actual' and 'Predicted' columns.\")\n",
    "\n",
    "# Plotting Actual vs. Predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Actual'], label='Actual', color='blue', linewidth=2)\n",
    "plt.plot(df['Predicted'], label='Predicted', color='orange', linestyle='--', linewidth=2)\n",
    "plt.title('Actual vs Predicted Values (Random Forest Regressor)', fontsize=14)\n",
    "plt.xlabel('Index', fontsize=12)\n",
    "plt.ylabel('Values', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage difference\n",
    "df['Difference (%)'] = ((df['Actual'] - df['Predicted']).abs() / df['Actual']) * 100\n",
    "\n",
    "# Print summary statistics for percentage difference\n",
    "mean_diff = df['Difference (%)'].mean()\n",
    "max_diff = df['Difference (%)'].max()\n",
    "min_diff = df['Difference (%)'].min()\n",
    "\n",
    "print(\"Percentage Difference Summary:\")\n",
    "print(f\"Mean Difference: {mean_diff:.2f}%\")\n",
    "print(f\"Max Difference: {max_diff:.2f}%\")\n",
    "print(f\"Min Difference: {min_diff:.2f}%\")\n",
    "\n",
    "# Optionally save the updated CSV with the difference column\n",
    "output_path = r\"C:\\Users\\viraj\\BuildSpace\\Projects\\stat429_project\\results\\predictions_with_diff.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Updated predictions with difference saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
